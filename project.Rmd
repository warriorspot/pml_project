# Predicting Exercise Skill
## Coursera Practical Machine Learning
### Brennan Cleveland

### Overview

The goal of this exercise is to predict the class of unlabeled exercise participants using labelled 
data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 
Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

I leveraged the caret package to build and compare two different models as follows:<br>

```{r fetch files, echo=FALSE}

options(warn=-1)

if(file.exists("pml-training.csv") == FALSE) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                  destfile="pml-training.csv")
}

if(file.exists("pml-testing.csv") == FALSE) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                  destfile="pml-testing.csv")

}

```
1. Create training and testing data partitions from the testing data set:

```{r create data partitions, echo=TRUE}

library(caret)

set.seed(1234)
data <- read.csv("./pml-training.csv", stringsAsFactors=FALSE)

inTrain <- createDataPartition(y=data$classe, p=0.7, list=FALSE)
training <- data[inTrain,]
testing <- data[-inTrain,]
```
```{r echo=FALSE}
# Out of sample error rate
outOfSampleError <- function(m) {
    pTesting <- predict(m, newdata=testing[,-160])
    c <- data.frame(actual=testing$classe, predicted=pTesting)
    errorRate <- nrow(c[c$actual != c$predicted,])/nrow(c)
}

```

2. I visually inspected the features in the training data by opening the file in a spreadsheet.
3. I noticed many features with largely missing or NA data. I decided to remove these.
4. I removed any non-numeric features.
5. There are a few 'book keeping' features (timestamp, etc) that I felt would not be needed.  I removed these.

I chose to pre-process the data by centering and scaling the remaning predictors, and performing Principle Component Analysis to attempt some data compression.  I used 3-fold cross validation to generate an in-sample error rate.

```{r clean data, echo=TRUE}
#Get rid of the first four columns, which dont contain measurement data
tclean <- subset(training, select=seq(5, ncol(training)))

# Consider only numeric predictors with no NA values
classe <- tclean$classe
tclean <- tclean[, sapply(tclean, is.numeric)]
tclean <- tclean[, colSums(is.na(tclean)) == 0]

preMethod <- c("center", "scale", "pca")
preObj <- preProcess(tclean, method=preMethod)
print(preObj)

tclean$classe <- as.factor(classe)
```

6. For model selection, I chose Linear Disciminant Analysis due to it's affinity for non-binary classification, and Random Forest, for it's accuracy.

```{r build the model, cache=TRUE, echo=TRUE}

tControl <- trainControl(method="cv", number=3)

mLda <- train(classe ~ ., method="lda", preProcess=c("center", "scale", "pca"),
           trainControl=tControl, data=tclean)
print(mLda)
errorRate <- outOfSampleError(mLda)
print(errorRate)

mRf <- train(classe ~ ., method="rf", preProcess=c("center", "scale", "pca"),
           trainControl=tControl, data=tclean)
print(mRf)
errorRate <- outOfSampleError(mRf)
print(errorRate)
```

The error rate for LDA is much too high.  The Random Forest model took much longer to generate, but has a much higher accuracy.  I used the Random Forest model for my final predictions.

```{r do final predictions, echo=TRUE}
#Try predictions on testing data
testData <- read.csv("./pml-testing.csv", stringsAsFactors=FALSE)
pFinal <- predict(mRf, newdata=testData)
print(data.frame(pFinal))
```
Using this model 19 of 20 predictions were correct.
